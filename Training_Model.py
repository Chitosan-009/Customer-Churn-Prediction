import pandas as pd
import numpy as np
import seaborn as sns
import matplotlib.pyplot as plt
from scipy.stats import randint, uniform
from sklearn.ensemble import RandomForestClassifier
from sklearn.model_selection import RandomizedSearchCV
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import (accuracy_score, precision_score, recall_score,
                           f1_score, roc_auc_score, confusion_matrix, roc_curve)
import lightgbm as lgb
import xgboost as xgb
import joblib
import warnings
from Data_Preprocessing import (load_data,processing_train, create_bin_features, data_quality_check,
                                create_derived_features, delete_col, test_processing, update_columns,
                                encode_data, feature_select, train_data_split)

warnings.filterwarnings('ignore')

def evaluate_model(y_true, y_pred, y_prob, model_name):
    """è¯„ä¼°å•ä¸ªæ¨¡å‹ï¼Œè¾“å‡ºå…³é”®æŒ‡æ ‡+å¹¶æ’å±•ç¤ºæ··æ·†çŸ©é˜µå’ŒROCæ›²çº¿"""
    print(f"\n{'=' * 50}")
    print(f"ğŸ” {model_name} æ¨¡å‹è¯„ä¼°ç»“æœï¼š")
    print(f"{'=' * 50}")

    # è®¡ç®—æ ¸å¿ƒæŒ‡æ ‡
    accuracy = accuracy_score(y_true, y_pred)
    precision = precision_score(y_true, y_pred)
    recall = recall_score(y_true, y_pred)
    f1 = f1_score(y_true, y_pred)
    roc_auc = roc_auc_score(y_true, y_prob)

    # è¾“å‡ºæŒ‡æ ‡
    metrics_info = {
        "å‡†ç¡®ç‡ï¼ˆAccuracyï¼‰": f"{accuracy:.4f} â†’ æ•´ä½“é¢„æµ‹æ­£ç¡®ç‡",
        "ç²¾ç¡®ç‡ï¼ˆPrecisionï¼‰": f"{precision:.4f} â†’ é¢„æµ‹æµå¤±çš„äººä¸­ï¼Œå®é™…æµå¤±çš„æ¯”ä¾‹",
        "å¬å›ç‡ï¼ˆRecallï¼‰": f"{recall:.4f} â†’ çœŸå®æµå¤±çš„äººä¸­ï¼Œè¢«æ­£ç¡®è¯†åˆ«çš„æ¯”ä¾‹",
        "F1-score": f"{f1:.4f} â†’ ç²¾ç¡®ç‡å’Œå¬å›ç‡çš„å¹³è¡¡å€¼",
        "ROC-AUC": f"{roc_auc:.4f} â†’ æ¨¡å‹åŒºåˆ†èƒ½åŠ›"
    }

    for metric, desc in metrics_info.items():
        print(f"  - {metric}: {desc}")

    # å¯è§†åŒ–å±•ç¤º
    _plot_model_evaluation(y_true, y_pred, y_prob, model_name, roc_auc)

    # è¿”å›æ ¸å¿ƒæŒ‡æ ‡ï¼ˆç”¨äºåç»­å¯¹æ¯”ï¼‰
    return {
        'æ¨¡å‹åç§°': model_name,
        'å‡†ç¡®ç‡': accuracy,
        'ç²¾ç¡®ç‡': precision,
        'å¬å›ç‡': recall,
        'F1-score': f1,
        'ROC-AUC': roc_auc,
        'éªŒè¯é›†é¢„æµ‹æ¦‚ç‡': y_prob
    }

def _plot_model_evaluation(y_true, y_pred, y_prob, model_name, roc_auc):
    """ç»˜åˆ¶æ¨¡å‹è¯„ä¼°å›¾è¡¨"""
    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))

    # 1. æ··æ·†çŸ©é˜µ
    cm = confusion_matrix(y_true, y_pred)
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['æœªæµå¤±ï¼ˆ0ï¼‰', 'æµå¤±ï¼ˆ1ï¼‰'],
                yticklabels=['æœªæµå¤±ï¼ˆ0ï¼‰', 'æµå¤±ï¼ˆ1ï¼‰'],
                ax=ax1)
    ax1.set_xlabel('é¢„æµ‹æ ‡ç­¾', fontsize=12)
    ax1.set_ylabel('çœŸå®æ ‡ç­¾', fontsize=12)
    ax1.set_title(f'{model_name} - æ··æ·†çŸ©é˜µ', fontsize=14, pad=20)

    # 2. ROCæ›²çº¿
    fpr, tpr, _ = roc_curve(y_true, y_prob)
    ax2.plot(fpr, tpr, color='darkorange', lw=3, label=f'AUC = {roc_auc:.4f}')
    ax2.plot([0, 1], [0, 1], 'k--', lw=2, alpha=0.7)  # éšæœºçŒœæµ‹åŸºå‡†çº¿
    ax2.set_xlim([0.0, 1.0])
    ax2.set_ylim([0.0, 1.05])
    ax2.set_xlabel('å‡é˜³æ€§ç‡ï¼ˆFalse Positive Rateï¼‰', fontsize=12)
    ax2.set_ylabel('çœŸé˜³æ€§ç‡ï¼ˆTrue Positive Rateï¼‰', fontsize=12)
    ax2.set_title(f'{model_name} - ROCæ›²çº¿', fontsize=14, pad=20)
    ax2.legend(loc="lower right", fontsize=11)
    ax2.grid(True, alpha=0.3)

    plt.tight_layout()
    plt.show()

def explain_logistic_regression(model, model_name, feature_names):
    """è§£é‡Šé€»è¾‘å›å½’æ¨¡å‹"""
    coefs = pd.DataFrame({
        'ç‰¹å¾åç§°': feature_names,
        'ç³»æ•°å€¼': model.coef_[0]  # é€»è¾‘å›å½’ç³»æ•° shape=(1, n_features)
    }).sort_values('ç³»æ•°å€¼', key=abs, ascending=False)

    print("ç‰¹å¾ç³»æ•°è§£é‡Šï¼š")
    print("  æ­£ç³»æ•° â†’ ç‰¹å¾å€¼è¶Šå¤§ï¼Œæµå¤±æ¦‚ç‡è¶Šé«˜")
    print("  è´Ÿç³»æ•° â†’ ç‰¹å¾å€¼è¶Šå¤§ï¼Œæµå¤±æ¦‚ç‡è¶Šä½")
    print("\nTop 10 é‡è¦ç‰¹å¾ï¼š")
    print(coefs.round(4).head(10))

    # å¯è§†åŒ–ç³»æ•°
    plt.figure(figsize=(12, 6))
    top_coefs = coefs.head(15)
    colors = ['red' if c > 0 else 'green' for c in top_coefs['ç³»æ•°å€¼']]

    bars = plt.barh(range(len(top_coefs)), top_coefs['ç³»æ•°å€¼'], color=colors, alpha=0.7)
    plt.yticks(range(len(top_coefs)), top_coefs['ç‰¹å¾åç§°'])
    plt.xlabel('ç³»æ•°å€¼', fontsize=12)
    plt.title(f'{model_name} - Top 15 ç‰¹å¾ç³»æ•°\n(çº¢è‰²=æ­£å‘å½±å“ï¼Œç»¿è‰²=è´Ÿå‘å½±å“)', fontsize=14, pad=20)
    plt.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, coef) in enumerate(zip(bars, top_coefs['ç³»æ•°å€¼'])):
        plt.text(coef + (0.01 if coef >= 0 else -0.01), i, f'{coef:.3f}',
                 ha='left' if coef >= 0 else 'right', va='center', fontsize=9)

    plt.tight_layout()
    plt.show()

def explain_tree_model(model, model_name, feature_names):
    """è§£é‡Šæ ‘æ¨¡å‹"""
    importances = pd.DataFrame({
        'ç‰¹å¾åç§°': feature_names,
        'é‡è¦æ€§å¾—åˆ†': model.feature_importances_
    }).sort_values('é‡è¦æ€§å¾—åˆ†', ascending=False)

    print("ç‰¹å¾é‡è¦æ€§æ’åï¼ˆå¾—åˆ†è¶Šé«˜ï¼Œå¯¹é¢„æµ‹è¶Šå…³é”®ï¼‰ï¼š")
    print("\nTop 10 é‡è¦ç‰¹å¾ï¼š")
    print(importances.round(4).head(10))

    # å¯è§†åŒ–ç‰¹å¾é‡è¦æ€§
    plt.figure(figsize=(12, 6))
    top_importances = importances.head(10)

    bars = plt.barh(range(len(top_importances)), top_importances['é‡è¦æ€§å¾—åˆ†'],
                    color='orange', alpha=0.7)
    plt.yticks(range(len(top_importances)), top_importances['ç‰¹å¾åç§°'])
    plt.xlabel('ç‰¹å¾é‡è¦æ€§å¾—åˆ†', fontsize=12)
    plt.title(f'{model_name} - Top 10 ç‰¹å¾é‡è¦æ€§', fontsize=14, pad=20)
    plt.grid(True, alpha=0.3, axis='x')

    # æ·»åŠ æ•°å€¼æ ‡ç­¾
    for i, (bar, score) in enumerate(zip(bars, top_importances['é‡è¦æ€§å¾—åˆ†'])):
        plt.text(score + 0.001, i, f'{score:.3f}', va='center', fontsize=10)

    plt.tight_layout()
    plt.show()

def explain_model_independent(model, model_name, feature_names):
    """ç‹¬ç«‹æ¨¡å‹è§£é‡Šå‡½æ•° - å•ç‹¬æŸ¥çœ‹æ¨¡å‹çš„ç‰¹å¾å½±å“"""
    print(f"\n{model_name} æ¨¡å‹ç‰¹å¾è§£é‡Š")
    print("-" * 50)

    # é€»è¾‘å›å½’ â†’ è¾“å‡ºç³»æ•°ï¼ˆæ­£è´Ÿå‘å½±å“ï¼‰
    if 'é€»è¾‘å›å½’' in model_name:
        explain_logistic_regression(model, model_name, feature_names)
    # æ ‘æ¨¡å‹ â†’ è¾“å‡ºç‰¹å¾é‡è¦æ€§
    else:
        explain_tree_model(model, model_name, feature_names)

def train_logistic_regression(X_train_scaled, y_train, X_val_scaled, y_val, selected_features):
    """è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹"""
    print("\nå¼€å§‹è®­ç»ƒé€»è¾‘å›å½’æ¨¡å‹...")
    print("=" * 50)

    # æ¨¡å‹è®­ç»ƒ
    lr_model = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)
    lr_model.fit(X_train_scaled, y_train)

    # éªŒè¯é›†é¢„æµ‹
    y_val_pred = lr_model.predict(X_val_scaled)
    y_val_prob = lr_model.predict_proba(X_val_scaled)[:, 1]

    # è¯„ä¼°æ¨¡å‹
    lr_metrics = evaluate_model(y_val, y_val_pred, y_val_prob, "é€»è¾‘å›å½’")

    # ä¿å­˜æ¨¡å‹æ–‡ä»¶
    joblib.dump(lr_model, 'é€»è¾‘å›å½’æ¨¡å‹.pkl')
    print("é€»è¾‘å›å½’æ¨¡å‹å·²ä¿å­˜ä¸º 'é€»è¾‘å›å½’æ¨¡å‹.pkl'")

    # æ¨¡å‹è§£é‡Š
    explain_model_independent(lr_model, "é€»è¾‘å›å½’", selected_features)

    return lr_model, lr_metrics

def train_random_forest(X_train_scaled, y_train, X_val_scaled, y_val, selected_features):
    """è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹ï¼ˆå¸¦è¶…å‚æ•°è°ƒä¼˜ï¼‰"""
    print("\nå¼€å§‹è®­ç»ƒéšæœºæ£®æ—æ¨¡å‹...")
    print("=" * 50)

    # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
    rf_param_dist = {
        'n_estimators': randint(100, 300),
        'max_depth': randint(5, 15),
        'min_samples_split': randint(2, 10),
        'min_samples_leaf': randint(1, 5)
    }

    print("ğŸ”§ æ­£åœ¨è¿›è¡Œéšæœºæœç´¢è°ƒå‚...")
    rf_random = RandomizedSearchCV(
        estimator=RandomForestClassifier(random_state=42, n_jobs=-1),
        param_distributions=rf_param_dist,
        n_iter=20,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    rf_random.fit(X_train_scaled, y_train)

    # æå–æœ€ä¼˜æ¨¡å‹
    best_rf = rf_random.best_estimator_
    print(f"éšæœºæ£®æ—æœ€ä¼˜å‚æ•°ï¼š{rf_random.best_params_}")
    print(f"æœ€ä¼˜äº¤å‰éªŒè¯AUCï¼š{rf_random.best_score_:.4f}")

    # éªŒè¯é›†é¢„æµ‹
    y_val_pred = best_rf.predict(X_val_scaled)
    y_val_prob = best_rf.predict_proba(X_val_scaled)[:, 1]

    # è¯„ä¼°æ¨¡å‹
    rf_metrics = evaluate_model(y_val, y_val_pred, y_val_prob, "è°ƒä¼˜ç‰ˆéšæœºæ£®æ—")

    # ä¿å­˜æ¨¡å‹æ–‡ä»¶
    joblib.dump(best_rf, 'éšæœºæ£®æ—æœ€ä¼˜æ¨¡å‹.pkl')
    print("éšæœºæ£®æ—æ¨¡å‹å·²ä¿å­˜ä¸º 'éšæœºæ£®æ—æœ€ä¼˜æ¨¡å‹.pkl'")

    # æ¨¡å‹è§£é‡Š
    explain_model_independent(best_rf, "éšæœºæ£®æ—", selected_features)

    return best_rf, rf_metrics

def train_xgboost(X_train_scaled, y_train, X_val_scaled, y_val, selected_features):
    """è®­ç»ƒXGBoostæ¨¡å‹ï¼ˆå¸¦è¶…å‚æ•°è°ƒä¼˜ï¼‰"""
    print("\nå¼€å§‹è®­ç»ƒXGBoostæ¨¡å‹...")
    print("=" * 50)

    # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
    xgb_param_dist = {
        'n_estimators': randint(100, 300),
        'learning_rate': uniform(0.01, 0.2),
        'max_depth': randint(3, 10),
        'subsample': uniform(0.7, 0.3),
        'colsample_bytree': uniform(0.7, 0.3)
    }

    print("æ­£åœ¨è¿›è¡Œéšæœºæœç´¢è°ƒä¼˜...")
    xgb_random = RandomizedSearchCV(
        estimator=xgb.XGBClassifier(random_state=42, objective='binary:logistic',
                                    eval_metric='auc', n_jobs=-1),
        param_distributions=xgb_param_dist,
        n_iter=20,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    xgb_random.fit(X_train_scaled, y_train)

    # æå–æœ€ä¼˜æ¨¡å‹
    best_xgb = xgb_random.best_estimator_
    print(f"XGBoostæœ€ä¼˜å‚æ•°ï¼š{xgb_random.best_params_}")
    print(f"æœ€ä¼˜äº¤å‰éªŒè¯AUCï¼š{xgb_random.best_score_:.4f}")

    # éªŒè¯é›†é¢„æµ‹
    y_val_pred = best_xgb.predict(X_val_scaled)
    y_val_prob = best_xgb.predict_proba(X_val_scaled)[:, 1]

    # è¯„ä¼°æ¨¡å‹
    xgb_metrics = evaluate_model(y_val, y_val_pred, y_val_prob, "XGBoost")

    # ä¿å­˜æ¨¡å‹æ–‡ä»¶
    joblib.dump(best_xgb, 'XGBoostæœ€ä¼˜æ¨¡å‹.pkl')
    print("XGBoostæ¨¡å‹å·²ä¿å­˜ä¸º 'XGBoostæœ€ä¼˜æ¨¡å‹.pkl'")

    # æ¨¡å‹è§£é‡Š
    explain_model_independent(best_xgb, "XGBoost", selected_features)

    return best_xgb, xgb_metrics

def train_lightgbm(X_train_scaled, y_train, X_val_scaled, y_val, selected_features):
    """è®­ç»ƒLightGBMæ¨¡å‹ï¼ˆå¸¦è¶…å‚æ•°è°ƒä¼˜ï¼‰"""
    print("\nå¼€å§‹è®­ç»ƒLightGBMæ¨¡å‹...")
    print("=" * 50)

    # å®šä¹‰å‚æ•°æœç´¢ç©ºé—´
    lgb_param_dist = {
        'n_estimators': randint(100, 300),
        'learning_rate': uniform(0.01, 0.2),
        'max_depth': randint(3, 10),
        'subsample': uniform(0.7, 0.3),
        'colsample_bytree': uniform(0.7, 0.3)
    }

    print("æ­£åœ¨è¿›è¡Œéšæœºæœç´¢è°ƒä¼˜...")
    lgb_random = RandomizedSearchCV(
        estimator=lgb.LGBMClassifier(random_state=42, objective='binary',
                                     metric='auc', n_jobs=-1),
        param_distributions=lgb_param_dist,
        n_iter=20,
        cv=5,
        scoring='roc_auc',
        n_jobs=-1,
        random_state=42,
        verbose=1
    )
    lgb_random.fit(X_train_scaled, y_train)

    # æå–æœ€ä¼˜æ¨¡å‹
    best_lgb = lgb_random.best_estimator_
    print(f"LightGBMæœ€ä¼˜å‚æ•°ï¼š{lgb_random.best_params_}")
    print(f"æœ€ä¼˜äº¤å‰éªŒè¯AUCï¼š{lgb_random.best_score_:.4f}")

    # éªŒè¯é›†é¢„æµ‹
    y_val_pred = best_lgb.predict(X_val_scaled)
    y_val_prob = best_lgb.predict_proba(X_val_scaled)[:, 1]

    # è¯„ä¼°æ¨¡å‹
    lgb_metrics = evaluate_model(y_val, y_val_pred, y_val_prob, "LightGBM")

    # ä¿å­˜æ¨¡å‹æ–‡ä»¶
    joblib.dump(best_lgb, 'LightGBMæœ€ä¼˜æ¨¡å‹.pkl')
    print("LightGBMæ¨¡å‹å·²ä¿å­˜ä¸º 'LightGBMæœ€ä¼˜æ¨¡å‹.pkl'")

    # æ¨¡å‹è§£é‡Š
    explain_model_independent(best_lgb, "LightGBM", selected_features)

    return best_lgb, lgb_metrics

def _plot_model_comparison(metrics_df):
    """ç»˜åˆ¶æ¨¡å‹å¯¹æ¯”å›¾"""
    fig, ((ax1, ax2), (ax3, ax4)) = plt.subplots(2, 2, figsize=(16, 12))

    # 1. ROC-AUC å¯¹æ¯”
    models = metrics_df['æ¨¡å‹åç§°']
    auc_scores = metrics_df['ROC-AUC']
    bars1 = ax1.bar(models, auc_scores, color='lightblue', alpha=0.8, edgecolor='navy')
    ax1.set_title('æ¨¡å‹ ROC-AUC å¯¹æ¯”', fontsize=14, pad=20)
    ax1.set_ylabel('ROC-AUC å¾—åˆ†', fontsize=12)
    ax1.set_ylim(0, 1)
    ax1.grid(True, alpha=0.3, axis='y')

    # åœ¨æŸ±å­ä¸Šæ·»åŠ æ•°å€¼
    for bar, score in zip(bars1, auc_scores):
        ax1.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                 f'{score:.4f}', ha='center', va='bottom', fontsize=10)

    # 2. F1-score å¯¹æ¯”
    f1_scores = metrics_df['F1-score']
    bars2 = ax2.bar(models, f1_scores, color='lightcoral', alpha=0.8, edgecolor='darkred')
    ax2.set_title('æ¨¡å‹ F1-score å¯¹æ¯”', fontsize=14, pad=20)
    ax2.set_ylabel('F1-score å¾—åˆ†', fontsize=12)
    ax2.set_ylim(0, 1)
    ax2.grid(True, alpha=0.3, axis='y')

    for bar, score in zip(bars2, f1_scores):
        ax2.text(bar.get_x() + bar.get_width() / 2, bar.get_height() + 0.01,
                 f'{score:.4f}', ha='center', va='bottom', fontsize=10)

    # 3. ç²¾ç¡®ç‡ vs å¬å›ç‡ æ•£ç‚¹å›¾
    ax3.scatter(metrics_df['ç²¾ç¡®ç‡'], metrics_df['å¬å›ç‡'], s=100, alpha=0.7,
                c=metrics_df['ROC-AUC'], cmap='viridis')
    ax3.set_xlabel('ç²¾ç¡®ç‡ (Precision)', fontsize=12)
    ax3.set_ylabel('å¬å›ç‡ (Recall)', fontsize=12)
    ax3.set_title('ç²¾ç¡®ç‡ vs å¬å›ç‡ (é¢œè‰²æ·±æµ…è¡¨ç¤ºAUC)', fontsize=14, pad=20)
    ax3.grid(True, alpha=0.3)

    # æ·»åŠ æ¨¡å‹æ ‡ç­¾
    for i, model in enumerate(models):
        ax3.annotate(model, (metrics_df['ç²¾ç¡®ç‡'][i], metrics_df['å¬å›ç‡'][i]),
                     xytext=(5, 5), textcoords='offset points', fontsize=9)

    # 4. ç»¼åˆæŒ‡æ ‡é›·è¾¾å›¾ï¼ˆç®€åŒ–ç‰ˆï¼‰
    metrics_to_plot = ['å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1-score', 'ROC-AUC']
    n_metrics = len(metrics_to_plot)

    angles = np.linspace(0, 2 * np.pi, n_metrics, endpoint=False).tolist()
    angles += angles[:1]  # é—­åˆé›·è¾¾å›¾

    for i, model in enumerate(models):
        values = metrics_df[metrics_to_plot].iloc[i].tolist()
        values += values[:1]  # é—­åˆé›·è¾¾å›¾
        ax4.plot(angles, values, 'o-', linewidth=2, label=model, markersize=4)
        ax4.fill(angles, values, alpha=0.1)

    ax4.set_xticks(angles[:-1])
    ax4.set_xticklabels(metrics_to_plot, fontsize=10)
    ax4.set_ylim(0, 1)
    ax4.set_title('æ¨¡å‹ç»¼åˆæ€§èƒ½é›·è¾¾å›¾', fontsize=14, pad=20)
    ax4.legend(loc='upper right', bbox_to_anchor=(1.3, 1.0))
    ax4.grid(True)

    plt.tight_layout()
    plt.show()

def _recommend_best_model(metrics_df, models_dict):
    """æ¨èæœ€ä¼˜æ¨¡å‹"""
    best_model = metrics_df.iloc[0]
    second_model = metrics_df.iloc[1] if len(metrics_df) > 1 else None

    print(f"\næœ€ä¼˜æ¨¡å‹æ¨è")
    print("=" * 50)
    print(f"æ¨èæ¨¡å‹ï¼š{best_model['æ¨¡å‹åç§°']}")
    print(f"ROC-AUCï¼š{best_model['ROC-AUC']:.4f}ï¼ˆæœ€é«˜ï¼‰")
    print(f"F1-scoreï¼š{best_model['F1-score']:.4f}")

    if second_model is not None:
        auc_improvement = best_model['ROC-AUC'] - second_model['ROC-AUC']
        print(f"ç›¸æ¯”ç¬¬äºŒå {second_model['æ¨¡å‹åç§°']}ï¼ŒAUCæå‡ï¼š{auc_improvement:.4f}")

    print(f"\næ¨èç†ç”±ï¼š")
    print("  - ç»¼åˆåŒºåˆ†èƒ½åŠ›ï¼ˆAUCï¼‰å’Œåˆ†ç±»å¹³è¡¡èƒ½åŠ›ï¼ˆF1ï¼‰æœ€ä¼˜")
    print("  - åœ¨ç²¾ç¡®ç‡å’Œå¬å›ç‡ä¹‹é—´å–å¾—è‰¯å¥½å¹³è¡¡")
    print("  - æ¨¡å‹ç¨³å®šæ€§å’Œæ³›åŒ–èƒ½åŠ›è¾ƒå¼º")

    # ä¿å­˜æœ€ä¼˜æ¨¡å‹ä¿¡æ¯
    best_model_info = {
        'model_name': best_model['æ¨¡å‹åç§°'],
        'model': models_dict[best_model['æ¨¡å‹åç§°']],
        'metrics': best_model.to_dict()
    }

    joblib.dump(best_model_info, 'æœ€ä¼˜æ¨¡å‹ä¿¡æ¯.pkl')
    print(f"\næœ€ä¼˜æ¨¡å‹ä¿¡æ¯å·²ä¿å­˜ä¸º 'æœ€ä¼˜æ¨¡å‹ä¿¡æ¯.pkl'")

def compare_models(model_metrics, models_dict):
    """å¯¹æ¯”æ‰€æœ‰æ¨¡å‹çš„æ€§èƒ½"""
    if not model_metrics:
        print("æ²¡æœ‰å¯å¯¹æ¯”çš„æ¨¡å‹æŒ‡æ ‡")
        return

    print("\nå››å¤§æ¨¡å‹æ ¸å¿ƒæŒ‡æ ‡æ±‡æ€»å¯¹æ¯”")
    print("=" * 80)

    # è½¬æ¢ä¸ºDataFrameå¹¶æ’åº
    metrics_df = pd.DataFrame(model_metrics)
    metrics_df = metrics_df[['æ¨¡å‹åç§°', 'å‡†ç¡®ç‡', 'ç²¾ç¡®ç‡', 'å¬å›ç‡', 'F1-score', 'ROC-AUC']]
    metrics_df = metrics_df.round(4)
    metrics_df_sorted = metrics_df.sort_values('ROC-AUC', ascending=False).reset_index(drop=True)

    # ç¾åŒ–è¾“å‡º
    print("æ¨¡å‹æ€§èƒ½å¯¹æ¯”è¡¨ï¼š")
    print(metrics_df_sorted.to_string(index=False))

    # å¯è§†åŒ–å¯¹æ¯”
    _plot_model_comparison(metrics_df_sorted)

    # è¾“å‡ºæœ€ä¼˜æ¨¡å‹æ¨è
    _recommend_best_model(metrics_df_sorted, models_dict)

def run_feature_engineering():
    """è¿è¡Œå®Œæ•´çš„ç‰¹å¾å·¥ç¨‹æµç¨‹"""
    print("å¼€å§‹ç‰¹å¾å·¥ç¨‹æµç¨‹...")
    print("=" * 50)

    try:
        # åŠ è½½æ•°æ®
        train_df = load_data("train.csv")
        train_df = data_quality_check(train_df, check_name="è®­ç»ƒé›†")
        train_df = processing_train(train_df)

        # åˆ›å»ºåˆ†ç®±ç‰¹å¾
        train_df, bin_boundaries = create_bin_features(train_df, is_train=True)
        bin_cols = [col for col in train_df.columns if col.endswith('_bin')]
        print(f"åˆ†ç®±ç‰¹å¾æ•°é‡ï¼š{len(bin_cols)}ä¸ª")

        # åˆ›å»ºè¡ç”Ÿç‰¹å¾
        train_df = create_derived_features(train_df)
        derived_cols = [col for col in train_df.columns if
                        'ratio' in col or 'is_' in col or 'value' in col or
                        'stability' in col or 'cost_per' in col]
        print(f"è¡ç”Ÿç‰¹å¾æ•°é‡ï¼š{len(derived_cols)}ä¸ª")
        print(f"è®­ç»ƒé›†ç‰¹å¾å·¥ç¨‹åå½¢çŠ¶: {train_df.shape}")

        # åˆ é™¤å†—ä½™å­—æ®µ
        train_df = delete_col(train_df)

        # å¤„ç†æµ‹è¯•é›†
        test_df = test_processing(bin_boundaries=bin_boundaries)
        test_df, train_df = update_columns(test_df, train_df)

        # ç¼–ç æ•°æ®
        train_df, test_df, train_df_encoded, test_df_encoded = encode_data(train_df, test_df)

        # ç‰¹å¾é€‰æ‹©
        X_train_selected, X_test_selected, X_train_full, y_train_full, selected_features = feature_select(
            train_df_encoded, test_df_encoded
        )

        # æ•°æ®æ‹†åˆ†å’Œæ ‡å‡†åŒ–
        X_train_scaled, X_val_scaled, X_test_scaled, y_train, y_val = train_data_split(
            X_train_selected, X_test_selected, y_train_full, selected_features
        )

        print("ç‰¹å¾å·¥ç¨‹å®Œæˆï¼")
        print(f"æœ€ç»ˆæ•°æ®ç»´åº¦:")
        print(f"  - è®­ç»ƒé›†: {X_train_scaled.shape}")
        print(f"  - éªŒè¯é›†: {X_val_scaled.shape}")
        print(f"  - æµ‹è¯•é›†: {X_test_scaled.shape}")
        print(f"  - é€‰ä¸­ç‰¹å¾æ•°: {len(selected_features)}")

        return {
            'X_train_scaled': X_train_scaled,
            'X_val_scaled': X_val_scaled,
            'X_test_scaled': X_test_scaled,
            'y_train': y_train,
            'y_val': y_val,
            'selected_features': selected_features
        }

    except Exception as e:
        print(f"ç‰¹å¾å·¥ç¨‹å¤±è´¥: {str(e)}")
        return None

def main():
    """ä¸»å‡½æ•° - æ‰§è¡Œå®Œæ•´çš„æ¨¡å‹è®­ç»ƒæµç¨‹"""
    print("å¼€å§‹å®¢æˆ·æµå¤±é¢„æµ‹æ¨¡å‹è®­ç»ƒæµç¨‹")
    print("=" * 60)

    try:
        # è¿è¡Œç‰¹å¾å·¥ç¨‹æµç¨‹
        feature_data = run_feature_engineering()
        if not feature_data:
            print("ç‰¹å¾å·¥ç¨‹å¤±è´¥ï¼Œç»ˆæ­¢æµç¨‹ï¼")
            return

        print("æ•°æ®å‡†å¤‡å®Œæˆï¼Œå¼€å§‹æ¨¡å‹è®­ç»ƒ...")

        # æå–ç‰¹å¾å·¥ç¨‹ç»“æœ
        X_train_scaled = feature_data['X_train_scaled']
        X_val_scaled = feature_data['X_val_scaled']
        X_test_scaled = feature_data['X_test_scaled']
        y_train = feature_data['y_train']
        y_val = feature_data['y_val']
        selected_features = feature_data['selected_features']

        # å­˜å‚¨æ¨¡å‹å’ŒæŒ‡æ ‡
        models_dict = {}
        model_metrics = []

        print("\n" + "æ¨¡å‹è®­ç»ƒé˜¶æ®µ ".ljust(50, "="))

        # è®­ç»ƒæ‰€æœ‰æ¨¡å‹
        # é€»è¾‘å›å½’
        lr_model, lr_metrics = train_logistic_regression(
            X_train_scaled, y_train, X_val_scaled, y_val, selected_features
        )
        models_dict['é€»è¾‘å›å½’'] = lr_model
        model_metrics.append(lr_metrics)

        # éšæœºæ£®æ—
        rf_model, rf_metrics = train_random_forest(
            X_train_scaled, y_train, X_val_scaled, y_val, selected_features
        )
        models_dict['éšæœºæ£®æ—'] = rf_model
        model_metrics.append(rf_metrics)

        # XGBoost
        xgb_model, xgb_metrics = train_xgboost(
            X_train_scaled, y_train, X_val_scaled, y_val, selected_features
        )
        models_dict['XGBoost'] = xgb_model
        model_metrics.append(xgb_metrics)

        # LightGBM
        lgb_model, lgb_metrics = train_lightgbm(
            X_train_scaled, y_train, X_val_scaled, y_val, selected_features
        )
        models_dict['LightGBM'] = lgb_model
        model_metrics.append(lgb_metrics)

        # æ¨¡å‹å¯¹æ¯”
        print("\n" + "æ¨¡å‹å¯¹æ¯”é˜¶æ®µ ".ljust(50, "="))
        compare_models(model_metrics, models_dict)

        print("\næ¨¡å‹è®­ç»ƒæµç¨‹å®Œæˆï¼")
        print("=" * 60)

        # è¾“å‡ºè®­ç»ƒæ€»ç»“
        print(f"è®­ç»ƒæ€»ç»“ï¼š")
        print(f"  - æˆåŠŸè®­ç»ƒæ¨¡å‹æ•°é‡ï¼š{len(models_dict)}")
        print(f"  - ä½¿ç”¨çš„ç‰¹å¾æ•°é‡ï¼š{len(selected_features)}")
        print(f"  - è®­ç»ƒé›†æ ·æœ¬æ•°ï¼š{X_train_scaled.shape[0]}")
        print(f"  - éªŒè¯é›†æ ·æœ¬æ•°ï¼š{X_val_scaled.shape[0]}")

    except Exception as e:
        print(f"æ¨¡å‹è®­ç»ƒè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯ï¼š{str(e)}")
        import traceback
        traceback.print_exc()

if __name__ == '__main__':
    main()